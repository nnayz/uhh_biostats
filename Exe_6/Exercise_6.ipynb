{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBdrxX7fIvPL"
      },
      "source": [
        "#Prequisites\n",
        "Install Python and required libraries for this exercise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKZQMiU283uj"
      },
      "source": [
        "#Federated Learning\n",
        "\n",
        "Your task is to implement a federated learning task by completing the code below.\n",
        "\n",
        "(10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP2cwOtI-wbp",
        "outputId": "5a7c4775-607e-4f30-969e-8b6529e113b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Federated Learning with 3 clients for 5 rounds\n",
            "\n",
            "--- Round 1/5 ---\n",
            "  Client 0: Loss = 0.7125\n",
            "  Client 1: Loss = 0.7235\n",
            "  Client 2: Loss = 0.6964\n",
            "  Evaluation:\n",
            "    Client 0: Accuracy = 30.00%\n",
            "    Client 1: Accuracy = 55.00%\n",
            "    Client 2: Accuracy = 60.00%\n",
            "\n",
            "--- Round 2/5 ---\n",
            "  Client 0: Loss = 0.7147\n",
            "  Client 1: Loss = 0.7108\n",
            "  Client 2: Loss = 0.6947\n",
            "  Evaluation:\n",
            "    Client 0: Accuracy = 35.00%\n",
            "    Client 1: Accuracy = 55.00%\n",
            "    Client 2: Accuracy = 60.00%\n",
            "\n",
            "--- Round 3/5 ---\n",
            "  Client 0: Loss = 0.7159\n",
            "  Client 1: Loss = 0.7204\n",
            "  Client 2: Loss = 0.6970\n",
            "  Evaluation:\n",
            "    Client 0: Accuracy = 35.00%\n",
            "    Client 1: Accuracy = 55.00%\n",
            "    Client 2: Accuracy = 60.00%\n",
            "\n",
            "--- Round 4/5 ---\n",
            "  Client 0: Loss = 0.7122\n",
            "  Client 1: Loss = 0.7200\n",
            "  Client 2: Loss = 0.6951\n",
            "  Evaluation:\n",
            "    Client 0: Accuracy = 35.00%\n",
            "    Client 1: Accuracy = 55.00%\n",
            "    Client 2: Accuracy = 60.00%\n",
            "\n",
            "--- Round 5/5 ---\n",
            "  Client 0: Loss = 0.7112\n",
            "  Client 1: Loss = 0.7128\n",
            "  Client 2: Loss = 0.6909\n",
            "  Evaluation:\n",
            "    Client 0: Accuracy = 35.00%\n",
            "    Client 1: Accuracy = 55.00%\n",
            "    Client 2: Accuracy = 60.00%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import copy\n",
        "\n",
        "# Simple Neural Network\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, input_size=10, hidden_size=20, num_classes=2):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "class Client:\n",
        "    def __init__(self, client_id, train_loader, test_loader):\n",
        "        self.client_id = client_id\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.model = SimpleNet()\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=0.01)\n",
        "\n",
        "    def get_parameters(self):\n",
        "        \"\"\"Get model parameters as a list\"\"\"\n",
        "        # TODO: Return model parameters as a list of tensors\n",
        "        # Hint: Use .clone() to avoid reference issues\n",
        "        return [param.clone() for param in self.model.parameters()]\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        \"\"\"Set model parameters from a list\"\"\"\n",
        "        # TODO: Update model parameters with the provided list\n",
        "        # Hint: Use zip() to pair model params with new params\n",
        "        for param, new_param in zip(self.model.parameters(), parameters):\n",
        "            param.data.copy_(new_param.data)\n",
        "\n",
        "    def train_one_epoch(self):\n",
        "        \"\"\"Train for one epoch on local data\"\"\"\n",
        "        # TODO: Implement training loop\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(data)\n",
        "            loss = self.criterion(output, target)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        return total_loss / len(self.train_loader)\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"Evaluate the model on test data\"\"\"\n",
        "        # TODO: Implement evaluation\n",
        "        self.model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in self.test_loader:\n",
        "                output = self.model(data)\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total += target.size(0)\n",
        "                correct += (predicted == target).sum().item()\n",
        "        accuracy = 100.0 * correct / total\n",
        "        return accuracy\n",
        "\n",
        "class Server:\n",
        "    def __init__(self):\n",
        "        self.global_model = SimpleNet()\n",
        "\n",
        "    def get_parameters(self):\n",
        "        \"\"\"Get global model parameters\"\"\"\n",
        "        # TODO\n",
        "        return [param.clone() for param in self.global_model.parameters()]\n",
        "\n",
        "    # Gets models from clients and averages their weights to create a better global model\n",
        "    def aggregate(self, client_parameters_list):\n",
        "        \"\"\"\n",
        "        Federated Averaging: Average parameters from all clients\n",
        "        \"\"\"\n",
        "        # TODO: Implement Federated Averaging (FedAvg)\n",
        "        # Average all client parameters\n",
        "        num_clients = len(client_parameters_list)\n",
        "        averaged_params = []\n",
        "        \n",
        "        # Initialize averaged params with zeros\n",
        "        for param in client_parameters_list[0]:\n",
        "            averaged_params.append(torch.zeros_like(param))\n",
        "        \n",
        "        # Sum all client parameters\n",
        "        for client_params in client_parameters_list:\n",
        "            for i, param in enumerate(client_params):\n",
        "                averaged_params[i] += param\n",
        "        \n",
        "        # Average by dividing by number of clients\n",
        "        for i in range(len(averaged_params)):\n",
        "            averaged_params[i] /= num_clients\n",
        "        \n",
        "        # Update global model with averaged parameters\n",
        "        for param, avg_param in zip(self.global_model.parameters(), averaged_params):\n",
        "            param.data.copy_(avg_param.data)\n",
        "\n",
        "def generate_client_data(num_samples=100, input_size=10):\n",
        "    \"\"\"Generate random data for a client\"\"\"\n",
        "    X = torch.randn(num_samples, input_size)\n",
        "    y = torch.randint(0, 2, (num_samples,))\n",
        "    dataset = TensorDataset(X, y)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    train_ds, test_ds = random_split(dataset, [train_size, len(dataset) - train_size])\n",
        "    return train_ds, test_ds\n",
        "\n",
        "def federated_learning(num_clients=3, num_rounds=5):\n",
        "    \"\"\"\n",
        "    Main federated learning loop\n",
        "    \"\"\"\n",
        "    # Initialize server\n",
        "    server = Server()\n",
        "\n",
        "    # TODO: Create clients with their own data\n",
        "    clients = []\n",
        "    for i in range(num_clients):\n",
        "        train_ds, test_ds = generate_client_data()\n",
        "        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "        test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
        "        clients.append(Client(i, train_loader, test_loader))\n",
        "\n",
        "    print(f\"Starting Federated Learning with {num_clients} clients for {num_rounds} rounds\\n\")\n",
        "\n",
        "    # Federated learning rounds\n",
        "    for round_num in range(num_rounds):\n",
        "        print(f\"--- Round {round_num + 1}/{num_rounds} ---\")\n",
        "\n",
        "        # Get global parameters\n",
        "        global_params = server.get_parameters()\n",
        "\n",
        "        # TODO: Train all clients and collect their parameters\n",
        "        client_params_list = []\n",
        "        for client in clients:\n",
        "            # Set global parameters to client\n",
        "            client.set_parameters(global_params)\n",
        "            # Train client for one epoch\n",
        "            loss = client.train_one_epoch()\n",
        "            print(f\"  Client {client.client_id}: Loss = {loss:.4f}\")\n",
        "            # Collect updated parameters\n",
        "            client_params_list.append(client.get_parameters())\n",
        "\n",
        "        # TODO: Aggregate client parameters on server\n",
        "        # Hint: Use server.aggregate()\n",
        "        server.aggregate(client_params_list)\n",
        "\n",
        "        # TODO: Evaluate all clients with new global model\n",
        "        print(\"  Evaluation:\")\n",
        "        global_params = server.get_parameters()\n",
        "        for client in clients:\n",
        "            client.set_parameters(global_params)\n",
        "            accuracy = client.evaluate()\n",
        "            print(f\"    Client {client.client_id}: Accuracy = {accuracy:.2f}%\")\n",
        "        print()\n",
        "\n",
        "# Run federated learning\n",
        "if __name__ == \"__main__\":\n",
        "    federated_learning(num_clients=3, num_rounds=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4h8UQ-M9PQJ"
      },
      "source": [
        "#Hugging Face\n",
        "\n",
        "In this exercise you are asked to complete the code to finetune a pretrained language model to classify a given review as positive or negative. This involves loading the pre-trained model, configuring basic training settings, connecting your data, and starting the training process. The tokenization function that converts text to numbers is already given.\n",
        "\n",
        "(5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS9pCxso9m16"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "\n",
        "# DISABLE ALL andb logging\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Configuration\n",
        "MODEL_CHECKPOINT = \"google-bert/bert-base-uncased\"\n",
        "NUM_LABELS = 2\n",
        "\n",
        "# Load SMALL dataset - only use 1000 examples for training!\n",
        "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
        "small_train = raw_datasets[\"train\"].select(range(1000))  # Only 1000 examples\n",
        "small_val = raw_datasets[\"validation\"].select(range(200))  # Only 200 examples\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize sentences for BERT\"\"\"\n",
        "    return tokenizer(examples[\"sentence\"], truncation=True)\n",
        "\n",
        "# TODO: Apply tokenization\n",
        "tokenized_train = small_train.map(tokenize_function, batched=True)\n",
        "tokenized_val = small_val.map(tokenize_function, batched=True)\n",
        "\n",
        "# TODO: Train the model\n",
        "def train_model():\n",
        "    # Load the pre-trained model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_CHECKPOINT, \n",
        "        num_labels=NUM_LABELS\n",
        "    )\n",
        "    \n",
        "    # Data collator for padding\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    \n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=10,\n",
        "        logging_dir=\"./logs\",\n",
        "    )\n",
        "    \n",
        "    # Create trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_val,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    \n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    \n",
        "    # Evaluate the model\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Evaluation results: {eval_results}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeITE4tw81bL"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
