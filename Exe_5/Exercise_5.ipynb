{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Group 1 : Diksha Prasad, Nasrul Huda, Omkar Kondhalkar, Aqsa Mohsin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBdrxX7fIvPL"
      },
      "source": [
        "#Foundation models\n",
        "\n",
        "#Prequisites\n",
        "Please install Python and required libraries for this exercise, or do it in google colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZSfnDONoH3r"
      },
      "source": [
        "# 1.\n",
        "Foundation models are pre-trained on large datasets to learn general representations. They can then be adapted to specific tasks through multiple methods: zero-shot/few-shot prompting, fine-tuning, or linear probing / feature extraction.\n",
        "\n",
        "Implement a foundation model that:\n",
        "- Pre-trains on a large general dataset to learn useful feature weights\n",
        "- Uses a) linear probing and b) full fine-tuning to train for a small task-specific dataset\n",
        "- Compare performance with training from scratch (random initialization)\n",
        "\n",
        "For simplicity reasons we use a simple linear regression model here.\n",
        "\n",
        "(15 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-0aeQx46CduD"
      },
      "outputs": [],
      "source": [
        "# These are the libraries we used\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size=3, hidden_size=4):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)\n",
        "        self.output = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sigmoid(self.hidden(x))\n",
        "        x = self.sigmoid(self.output(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_uVW2ufLDvSj"
      },
      "outputs": [],
      "source": [
        "# === Solution helper + training functions ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Helper to convert list-of-tuples data into tensors\n",
        "def prepare_tensors(data):\n",
        "    \"\"\"\n",
        "    data: list of (input_list, label) pairs\n",
        "    returns: X [N, 3], y [N, 1]\n",
        "    \"\"\"\n",
        "    X = torch.tensor([x for x, _ in data], dtype=torch.float32)\n",
        "    y = torch.tensor([[y] for _, y in data], dtype=torch.float32)\n",
        "    return X, y\n",
        "\n",
        "def pretrain(model, data, epochs, lr=0.1):\n",
        "    \"\"\"\n",
        "    Pretrain the model on the large, general dataset.\n",
        "    We train all parameters from random initialization.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    X, y = prepare_tensors(data)\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def linear_probing(model, data, epochs, lr=0.1):\n",
        "    \"\"\"\n",
        "    Freeze hidden layers, only train the output layer on the small, specific dataset.\n",
        "    This corresponds to linear probing: using the pretrained features as fixed.\n",
        "    \"\"\"\n",
        "    # Freeze hidden layer weights\n",
        "    for param in model.hidden.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Ensure output layer is trainable\n",
        "    for param in model.output.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    X, y = prepare_tensors(data)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.SGD(model.output.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def full_fine_tune(model, data, epochs, lr=0.1):\n",
        "    \"\"\"\n",
        "    Retrain all weights, but reset the last layer's weights first.\n",
        "    This corresponds to full fine-tuning of the pretrained model.\n",
        "    \"\"\"\n",
        "    # Reset last layer to break any bias inherited from pretraining\n",
        "    model.output.reset_parameters()\n",
        "\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    X, y = prepare_tensors(data)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_from_scratch(data, epochs, lr=0.1):\n",
        "    \"\"\"\n",
        "    Train a new model from scratch using only the task-specific data.\n",
        "    No pretraining here.\n",
        "    \"\"\"\n",
        "    model = SimpleNN()\n",
        "    X, y = prepare_tensors(data)\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, data):\n",
        "    \"\"\"\n",
        "    Evaluate a model on a dataset: returns accuracy and raw probabilities.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    X, y = prepare_tensors(data)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X)\n",
        "        preds = (outputs >= 0.5).float()\n",
        "        accuracy = (preds.eq(y).sum().item()) / len(data)\n",
        "\n",
        "    return accuracy, outputs.squeeze().tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KpEaYH0SEfYa"
      },
      "outputs": [],
      "source": [
        "general_data = [\n",
        "    ([1, 1, 1], 1), ([1, 1, 0], 1), ([1, 0, 1], 1),\n",
        "    ([0, 0, 0], 0), ([0, 0, 1], 0), ([0, 1, 0], 0),\n",
        "    ([1, 1, 1], 1), ([0, 1, 1], 0), ([1, 0, 0], 1),\n",
        "    ([0, 0, 0], 0), ([1, 1, 0], 1), ([0, 1, 1], 0),\n",
        "]\n",
        "\n",
        "specific_data = [\n",
        "    ([1, 1, 1], 1), ([0, 0, 0], 0),\n",
        "    ([1, 1, 0], 1), ([0, 1, 0], 0),\n",
        "]\n",
        "\n",
        "# Pretrain the model on the general data and use both transfer learning techniques\n",
        "# separately to retrain the model for the specified data only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ckjjoFuEFuS1"
      },
      "outputs": [],
      "source": [
        "# Train a new model from scratch using only the specified data and compare the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2875e43f",
        "outputId": "29f61fd7-2a7d-430d-ed20-59b27a02e519"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on specific data after linear probing:   1.00\n",
            "Accuracy on specific data after full fine-tuning: 1.00\n"
          ]
        }
      ],
      "source": [
        "# === Pretraining and transfer learning on general/specific data (appended) ===\n",
        "import copy\n",
        "\n",
        "# 1) Pretrain a base model on the general data\n",
        "base_model = SimpleNN()\n",
        "pretrain_epochs = 500\n",
        "pretrain(base_model, general_data, epochs=pretrain_epochs)\n",
        "\n",
        "# 2) Create two copies for the two transfer-learning strategies\n",
        "linear_model = copy.deepcopy(base_model)      # for linear probing\n",
        "finetune_model = copy.deepcopy(base_model)    # for full fine-tuning\n",
        "\n",
        "# 3) Apply linear probing (only output layer trained on specific_data)\n",
        "linear_epochs = 200\n",
        "linear_probing(linear_model, specific_data, epochs=linear_epochs)\n",
        "\n",
        "# 4) Apply full fine-tuning (all layers, last layer reset) on specific_data\n",
        "finetune_epochs = 200\n",
        "full_fine_tune(finetune_model, specific_data, epochs=finetune_epochs)\n",
        "\n",
        "# 5) Evaluate both models on the specific_data\n",
        "acc_linear, _ = evaluate(linear_model, specific_data)\n",
        "acc_finetune, _ = evaluate(finetune_model, specific_data)\n",
        "\n",
        "print(f\"Accuracy on specific data after linear probing:   {acc_linear:.2f}\")\n",
        "print(f\"Accuracy on specific data after full fine-tuning: {acc_finetune:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e531b18",
        "outputId": "c2f68669-1619-4ded-db22-d9ab0cadf317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on specific data when training from scratch: 1.00\n",
            "\n",
            "Comparison on task-specific data:\n",
            "  Linear probing (pretrained features):   1.00\n",
            "  Full fine-tuning (pretrained model):    1.00\n",
            "  Training from scratch (no pretraining): 1.00\n"
          ]
        }
      ],
      "source": [
        "# === Training from scratch on specific data and comparison (appended) ===\n",
        "scratch_epochs = 200\n",
        "scratch_model = train_from_scratch(specific_data, epochs=scratch_epochs)\n",
        "\n",
        "# Evaluate the scratch model\n",
        "acc_scratch, _ = evaluate(scratch_model, specific_data)\n",
        "\n",
        "print(f\"Accuracy on specific data when training from scratch: {acc_scratch:.2f}\")\n",
        "\n",
        "# Compare all three approaches\n",
        "print(\"\\nComparison on task-specific data:\")\n",
        "print(f\"  Linear probing (pretrained features):   {acc_linear:.2f}\")\n",
        "print(f\"  Full fine-tuning (pretrained model):    {acc_finetune:.2f}\")\n",
        "print(f\"  Training from scratch (no pretraining): {acc_scratch:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPW8pTPa_JJI"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "On this tiny, perfectly learnable dataset, all three approaches\n",
        "(linear probing, full fine-tuning, and training from scratch) reach\n",
        "100% accuracy. This is expected because the model is expressive enough\n",
        "and we train for enough epochs to perfectly fit the data.\n",
        "\n",
        "In a more realistic/high-dimensional setting, pretraining typically\n",
        "helps when we have limited task-specific data: linear probing and full\n",
        "fine-tuning often outperform training from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mlDhzMZ_Oon"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
