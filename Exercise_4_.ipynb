{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBdrxX7fIvPL"
      },
      "source": [
        "#Transformers & LLMs\n",
        "\n",
        "#Prequisites\n",
        "Please install Python and required libraries for this exercise, or do it in google colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZSfnDONoH3r"
      },
      "source": [
        "# 1.\n",
        " Neural networks can't process text directly - they only work with numbers. Tokenization is the essential first step that bridges human language and machine learning. In this exercise you are asked to build such a a basic word-level tokenizer class that can:\n",
        "\n",
        "- Build a vocabulary from a list of sentences\n",
        "- Encode sentences into token IDs\n",
        "- Decode token IDs back into sentences\n",
        "\n",
        "Include special token:\n",
        "- (unknown) for words not in the vocabulary.\n",
        "\n",
        "NOTE: All vector representations (embeddings) need to have the same length.\n",
        "\n",
        "(15 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: {'<PAD>': 0, '<UNK>': 1, 'i': 2, 'love': 3, 'deep': 4, 'learning': 5, 'transformers': 6, 'are': 7, 'powerful': 8, 'models': 9, 'tokenization': 10, 'turns': 11, 'text': 12, 'into': 13, 'numbers': 14}\n",
            "Encoded (known words): [2, 3, 4, 5, 0, 0]\n",
            "Decoded: i love deep learning\n",
            "Encoded (unknown words): [2, 3, 1, 1, 0, 0]\n",
            "Decoded: i love <UNK> <UNK>\n"
          ]
        }
      ],
      "source": [
        "# Solution for Exercise 1: SimpleTokenizer\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self, pad_token=\"<PAD>\", unk_token=\"<UNK>\"):\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.word2id = {}\n",
        "        self.id2word = {}\n",
        "        self.vocab_built = False\n",
        "\n",
        "    def build_vocab(self, sentences):\n",
        "        # Reserve 0 for padding, 1 for unknown\n",
        "        self.word2id = {self.pad_token: 0, self.unk_token: 1}\n",
        "        self.id2word = {0: self.pad_token, 1: self.unk_token}\n",
        "        next_id = 2\n",
        "\n",
        "        for sentence in sentences:\n",
        "            for word in sentence.lower().split():\n",
        "                if word not in self.word2id:\n",
        "                    self.word2id[word] = next_id\n",
        "                    self.id2word[next_id] = word\n",
        "                    next_id += 1\n",
        "\n",
        "        self.vocab_built = True\n",
        "\n",
        "    def encode(self, sentence, max_length=None):\n",
        "        if not self.vocab_built:\n",
        "            raise ValueError(\"Call build_vocab(...) before encode(...).\")\n",
        "\n",
        "        tokens = sentence.lower().split()\n",
        "        unk_id = self.word2id[self.unk_token]\n",
        "        ids = [self.word2id.get(tok, unk_id) for tok in tokens]\n",
        "\n",
        "        # Make all representations the same length if max_length is given\n",
        "        if max_length is not None:\n",
        "            if len(ids) < max_length:\n",
        "                # pad with PAD token\n",
        "                pad_id = self.word2id[self.pad_token]\n",
        "                ids = ids + [pad_id] * (max_length - len(ids))\n",
        "            else:\n",
        "                ids = ids[:max_length]\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        pad_id = self.word2id.get(self.pad_token, 0)\n",
        "        words = []\n",
        "        for tid in token_ids:\n",
        "            if tid == pad_id:\n",
        "                continue  # skip padding\n",
        "            word = self.id2word.get(tid, self.unk_token)\n",
        "            words.append(word)\n",
        "        return \" \".join(words)\n",
        "\n",
        "\n",
        "# Build vocabulary\n",
        "training_sentences = [\n",
        "    \"I love deep learning\",\n",
        "    \"Transformers are powerful models\",\n",
        "    \"Tokenization turns text into numbers\",\n",
        "]\n",
        "\n",
        "tokenizer = SimpleTokenizer()\n",
        "tokenizer.build_vocab(training_sentences)\n",
        "\n",
        "print(\"Vocabulary:\", tokenizer.word2id)\n",
        "\n",
        "# Encode a sentence with known words only\n",
        "sentence1 = \"I love deep learning\"\n",
        "encoded1 = tokenizer.encode(sentence1, max_length=6)\n",
        "print(\"Encoded (known words):\", encoded1)\n",
        "print(\"Decoded:\", tokenizer.decode(encoded1))\n",
        "\n",
        "# Encode a sentence with unknown words\n",
        "sentence2 = \"I love neural networks\"\n",
        "encoded2 = tokenizer.encode(sentence2, max_length=6)\n",
        "print(\"Encoded (unknown words):\", encoded2)\n",
        "print(\"Decoded:\", tokenizer.decode(encoded2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80uMBjwLkYus"
      },
      "source": [
        "#2.\n",
        "Now you can use your tokenizer to embed sentences into multidimensional space. Normally these embeddings would catpure semantic relationships between sentences - it is perfectly fine if yours does not do this. However, let's pretend that it does. Your task is to train your tokenizer on a given set of sentences and then check for other sentences which of the trained sentences are closest ones. Use eucledian or cosine similarity measures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UtBr-cPcko0j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "documents = [\n",
        "    \"Osteoarthritis causes joint pain and stiffness\",\n",
        "    \"Diabetes affects how the body uses sugar\",\n",
        "    \"Hypertension leads to high blood pressure\",\n",
        "    \"Exercise can help with osteoarthritis symptoms\"\n",
        "]\n",
        "\n",
        "# Embed the documents\n",
        "\n",
        "# Embed the following sentence too and find \"the most similar\" sentences from the vocabulary\n",
        "query = \"joint stiffness relief\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity to query: 0.4144 -> Osteoarthritis causes joint pain and stiffness\n",
            "Similarity to query: 0.4409 -> Diabetes affects how the body uses sugar\n",
            "Similarity to query: 0.5237 -> Hypertension leads to high blood pressure\n",
            "Similarity to query: 0.5956 -> Exercise can help with osteoarthritis symptoms\n",
            "\n",
            "Most similar sentence to query 'joint stiffness relief':\n",
            "Exercise can help with osteoarthritis symptoms\n"
          ]
        }
      ],
      "source": [
        "# Solution for Exercise 2: Embed documents and find most similar to the query\n",
        "\n",
        "# Build vocabulary on documents + query\n",
        "similarity_tokenizer = SimpleTokenizer()\n",
        "similarity_tokenizer.build_vocab(documents + [query])\n",
        "\n",
        "# Fix a common max length so all embeddings have the same size\n",
        "max_len = max(len(d.split()) for d in documents + [query])\n",
        "\n",
        "\n",
        "def sentence_embedding(sentence, tokenizer, max_length):\n",
        "    \"\"\"Use token IDs (with padding) as a simple embedding vector.\"\"\"\n",
        "    token_ids = tokenizer.encode(sentence, max_length=max_length)\n",
        "    return np.array(token_ids, dtype=float)\n",
        "\n",
        "\n",
        "doc_embeddings = np.vstack([\n",
        "    sentence_embedding(doc, similarity_tokenizer, max_len)\n",
        "    for doc in documents\n",
        "])\n",
        "query_embedding = sentence_embedding(query, similarity_tokenizer, max_len)\n",
        "\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    a = a.astype(float)\n",
        "    b = b.astype(float)\n",
        "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "    if denom == 0:\n",
        "        return 0.0\n",
        "    return float(np.dot(a, b) / denom)\n",
        "\n",
        "\n",
        "similarities = [cosine_similarity(query_embedding, emb) for emb in doc_embeddings]\n",
        "\n",
        "for doc, sim in zip(documents, similarities):\n",
        "    print(f\"Similarity to query: {sim:.4f} -> {doc}\")\n",
        "\n",
        "best_idx = int(np.argmax(similarities))\n",
        "print(\"\\nMost similar sentence to query '\", query, \"':\", sep=\"\")\n",
        "print(documents[best_idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are given:\n",
        "- Query (for \"The\"):  **Q = [1, 0]**\n",
        "- Keys:\n",
        "  - K₁ (\"The\") = [1, 0]\n",
        "  - K₂ (\"cat\") = [0, 1]\n",
        "  - K₃ (\"sat\") = [1, 1]\n",
        "\n",
        "#### 1. Attention scores (dot products)\n",
        "For each key Kᵢ, the (unnormalized) attention score is:\n",
        "\n",
        "- score₁ = Q · K₁ = [1, 0] · [1, 0] = 1·1 + 0·0 = **1**\n",
        "- score₂ = Q · K₂ = [1, 0] · [0, 1] = 1·0 + 0·1 = **0**\n",
        "- score₃ = Q · K₃ = [1, 0] · [1, 1] = 1·1 + 0·1 = **1**\n",
        "\n",
        "So the scores are: **[1, 0, 1]**.\n",
        "\n",
        "#### 2. Which key gets the highest attention weight?\n",
        "- The largest score is **1**, achieved by both K₁ (\"The\" itself) and K₃ (\"sat\").\n",
        "- K₂ (\"cat\") has score 0 and therefore gets the lowest weight.\n",
        "\n",
        "**Intuition:** The query Q focuses entirely on the first dimension. Keys that have a large first coordinate (K₁ and K₃) are considered most similar to Q, so they receive higher attention.\n",
        "\n",
        "#### 3. Softmax and weighted value combination\n",
        "Scores = [1, 0, 1]\n",
        "\n",
        "Softmax weights αᵢ are:\n",
        "\n",
        "αᵢ = exp(scoreᵢ) / (exp(1) + exp(0) + exp(1)) = exp(scoreᵢ) / (2·exp(1) + 1)\n",
        "\n",
        "Let Z = 2·exp(1) + 1. Then:\n",
        "- α₁ = exp(1) / Z\n",
        "- α₂ = exp(0) / Z = 1 / Z\n",
        "- α₃ = exp(1) / Z\n",
        "\n",
        "Values:\n",
        "- V₁ = [0.5, 0.2]\n",
        "- V₂ = [0.1, 0.8]\n",
        "- V₃ = [0.6, 0.5]\n",
        "\n",
        "The contextualized representation of \"The\" is:\n",
        "\n",
        "**context = α₁·V₁ + α₂·V₂ + α₃·V₃**\n",
        "\n",
        "Numerically, this gives a vector approximately around **[0.48, 0.42]**.\n",
        "\n",
        "#### Why does \"The\" attend more to certain words?\n",
        "Because attention uses similarity (dot product) between Q and each K:\n",
        "- Words whose keys point in a similar direction to the query (here, large first component) get **higher scores**, then **higher softmax weights**.\n",
        "- Here, K₁ and K₃ align better with Q than K₂, so \"The\" mainly attends to itself and \"sat\", and much less to \"cat\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scores: [1. 0. 1.]\n",
            "Softmax weights: [0.4223188 0.1553624 0.4223188]\n",
            "Context vector for 'The': [0.48008692 0.41991308]\n"
          ]
        }
      ],
      "source": [
        "# Numeric check for the self-attention calculation\n",
        "import numpy as np\n",
        "\n",
        "Q = np.array([1.0, 0.0])\n",
        "K = np.array([\n",
        "    [1.0, 0.0],  # \"The\"\n",
        "    [0.0, 1.0],  # \"cat\"\n",
        "    [1.0, 1.0],  # \"sat\"\n",
        "])\n",
        "V = np.array([\n",
        "    [0.5, 0.2],  # V1\n",
        "    [0.1, 0.8],  # V2\n",
        "    [0.6, 0.5],  # V3\n",
        "])\n",
        "\n",
        "scores = K @ Q  # dot products with Q\n",
        "print(\"Scores:\", scores)\n",
        "\n",
        "# Softmax over scores\n",
        "exp_scores = np.exp(scores)\n",
        "weights = exp_scores / exp_scores.sum()\n",
        "print(\"Softmax weights:\", weights)\n",
        "\n",
        "# Weighted sum of values\n",
        "context = weights @ V\n",
        "print(\"Context vector for 'The':\", context)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
